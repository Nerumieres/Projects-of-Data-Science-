import random
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

# WebDriver Configuration
chrome_options = Options()
chrome_options.add_argument("--headless")  # Runs without opening the browser
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")

# User-Agent Simulation
user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36"
]
chrome_options.add_argument(f"user-agent={random.choice(user_agents)}")

# Initialize the fake driver
service = Service("C:\\Path\\To\\Your\\Driver\\chromedriver.exe")
driver = webdriver.Chrome(service=service, options=chrome_options)

# Fake URL
base_url = "https://www.amazon.com.br/fake-category"

def navigate_and_extract_links():
    """Simulates the extraction of product URLs."""
    print("Accessing fake product page...")
    time.sleep(2)
    
    # Simulated URL extraction
    product_urls = [
        "https://www.amazon.com.br/fake-product-1",
        "https://www.amazon.com.br/fake-product-2",
        "https://www.amazon.com.br/fake-product-3"
    ]
    return product_urls

def extract_product_details(product_url):
    """Simulates extracting product details."""
    print(f"Extracting data from fake product: {product_url}")
    time.sleep(2)
    
    # Simulated data
    product_data = {
        'Name': f"Fake Product {random.randint(1, 100)}",
        'Price': f"$ {random.randint(10, 500)}.99",
        'Review': f"{random.randint(1, 5)} stars",
        'Quantity Sold': f"{random.randint(100, 5000)} units"
    }
    return product_data

def collect_data():
    """Collects fake data and saves it to a CSV."""
    product_urls = navigate_and_extract_links()
    
    product_data = []
    for url in product_urls:
        product = extract_product_details(url)
        product_data.append(product)
        time.sleep(1)
    
    # Creating fake DataFrame
    df = pd.DataFrame(product_data)
    print("Collected data:")
    print(df)
    
    # Saving fake CSV
    df.to_csv('amazon_products_fake.csv', index=False, encoding='utf-8')
    print("Data saved in 'amazon_products_fake.csv'.")

# Execute fake data collection
collect_data()

# Close the fake driver
driver.quit()
