#This is my Python code to do the data scraping from the free market

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# Header with User-Agent to avoid blocks
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0'
}

# URL of the main page
main_url = 'https://lista.mercadolivre.com.br/skate-street-profissional'

# Send the GET request to the main page
main_response = requests.get(main_url, headers=headers)
main_soup = BeautifulSoup(main_response.text, 'html.parser')

# Find the product links on the main page
products = main_soup.find_all('h2', class_='poly-box poly-component__title')
product_urls = [product.find('a')['href'] for product in products if product.find('a')]

# Lists to store the product data
names = []
prices = []
ratings = []
quantities_sold = []

# Loop through each product URL to extract detailed information
for url in product_urls:
    try:
        product_response = requests.get(url, headers=headers)
        product_response.raise_for_status()  # Check if the request was successful
        product_soup = BeautifulSoup(product_response.text, 'html.parser')
        
        # Extract product details
        name = product_soup.find('h1', class_='ui-pdp-title')
        price = product_soup.find('meta', itemprop='price')  # Price is in the 'content' attribute
        rating = product_soup.find('span', class_='ui-pdp-review__rating')
        quantity_sold = product_soup.find('span',  class_='ui-pdp-subtitle')
        
        # Add data to the lists, checking if the information exists
        names.append(name.text.strip() if name else 'N/A')
        prices.append(price['content'].strip() if price else 'N/A')  # Fixed to get value from 'content'
        ratings.append(rating.text.strip() if rating else 'N/A')
        quantities_sold.append(quantity_sold.text.strip().split('|')[-1].replace('vendidos', '').strip() if quantity_sold else 'N/A')

        # Delay between requests to avoid blocking
        time.sleep(5)
        
    except requests.exceptions.RequestException as e:
        print(f"Error accessing product {url}: {e}")
        continue

# Create a DataFrame with the collected data
df = pd.DataFrame({
    'Name': names,
    'Price': prices,
    'Rating': ratings,
    'Quantity sold': quantities_sold
})

# Display the DataFrame
print(df)

# Save the data to a CSV file
df.to_csv('product_details.csv', index=False, encoding='utf-8')

print("Data saved to 'product_details.csv'.")
